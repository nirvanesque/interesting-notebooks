{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e25c203",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nirvanesque/llama_index_methods/blob/main/query_pipeline_present.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf73ae-a632-4d09-b941-6739e35b760d",
   "metadata": {
    "id": "8ebf73ae-a632-4d09-b941-6739e35b760d"
   },
   "source": [
    "# An Introduction to LlamaIndex Query Pipelines\n",
    "\n",
    "## Overview\n",
    "LlamaIndex provides a declarative query API that allows you to chain together different modules in order to orchestrate simple-to-advanced workflows over your data.\n",
    "\n",
    "This is centered around our `QueryPipeline` abstraction. Load in a variety of modules (from LLMs to prompts to retrievers to other pipelines), connect them all together into a sequential chain or DAG, and run it end2end.\n",
    "- Link to llama_index QueryPipelines: https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline.html\n",
    "- Link to llama_index QueryPipelines (async): https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_async.html\n",
    "\n",
    "**NOTE**: You can orchestrate all these workflows without the declarative pipeline abstraction (by using the modules imperatively and writing your own functions). So what are the advantages of `QueryPipeline`?\n",
    "- Express common workflows with fewer lines of code/boilerplate\n",
    "- Greater readability\n",
    "- Greater parity / better integration points with common low-code / no-code solutions (e.g. LangFlow)\n",
    "- [In the future] A declarative interface allows easy serializability of pipeline components, providing portability of pipelines/easier deployment to different systems.\n",
    "\n",
    "## Cookbook\n",
    "\n",
    "In this cookbook we give you an introduction to our `QueryPipeline` interface and show you some basic workflows you can tackle.\n",
    "\n",
    "- Chain together prompt and LLM\n",
    "- Chain together query rewriting (prompt + LLM) with retrieval\n",
    "- Chain together a full RAG query pipeline (query rewriting, retrieval, reranking, response synthesis)\n",
    "- Setting up a custom query component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664df267-4285-4181-8ef7-1880177dc95f",
   "metadata": {
    "id": "664df267-4285-4181-8ef7-1880177dc95f"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade llama-index==0.9.45.post1 arize-phoenix==2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144b2d4-adbc-44da-8c12-bdb5fe4b18bb",
   "metadata": {
    "id": "1144b2d4-adbc-44da-8c12-bdb5fe4b18bb"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Here we setup some data + indexes (from PG's essay) that we'll be using in the rest of the cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82744-965a-4d79-b357-faf3de7ba2f8",
   "metadata": {
    "id": "adc82744-965a-4d79-b357-faf3de7ba2f8"
   },
   "outputs": [],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import phoenix as px\n",
    "px.launch_app()\n",
    "import llama_index\n",
    "llama_index.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fcb621-0894-457e-b602-dbf5fb9134ec",
   "metadata": {
    "id": "40fcb621-0894-457e-b602-dbf5fb9134ec"
   },
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import QueryPipeline\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009af96-59e3-4d14-8272-382203c8b8a7",
   "metadata": {
    "id": "2009af96-59e3-4d14-8272-382203c8b8a7"
   },
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"../data/paul_graham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d390b-38ca-4176-8cdb-8c2a0af1add8",
   "metadata": {
    "id": "c55d390b-38ca-4176-8cdb-8c2a0af1add8",
    "outputId": "7c239a3f-7b8d-44e3-ec76-378a9f757aad"
   },
   "outputs": [],
   "source": [
    "docs = reader.load_data()\n",
    "print(docs[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ddc81-0783-4fa5-ade0-60700c918011",
   "metadata": {
    "id": "780ddc81-0783-4fa5-ade0-60700c918011"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.storage import StorageContext\n",
    "\n",
    "if not os.path.exists(\"storage\"):\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c59b5c-9b18-4dfa-97ef-e39b8069b73c",
   "metadata": {
    "id": "d5c59b5c-9b18-4dfa-97ef-e39b8069b73c"
   },
   "source": [
    "## 1. Chain Together Prompt and LLM\n",
    "\n",
    "In this section we show a super simple workflow of chaining together a prompt with LLM.\n",
    "\n",
    "We simply define `chain` on initialization. This is a special case of a query pipeline where the components are purely sequential, and we automatically convert outputs into the right format for the next inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb233a0f-2993-4780-a241-6a2299047598",
   "metadata": {
    "id": "cb233a0f-2993-4780-a241-6a2299047598"
   },
   "outputs": [],
   "source": [
    "# try chaining basic prompts\n",
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26c0c6-b886-42a6-b524-b19b18d1c01c",
   "metadata": {
    "id": "1b26c0c6-b886-42a6-b524-b19b18d1c01c",
    "outputId": "906a8e07-fca4-4481-d844-17505e783f43"
   },
   "outputs": [],
   "source": [
    "output = p.run(movie_name=\"The Departed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987554f-4214-409f-97a8-40d2b13c25ee",
   "metadata": {
    "id": "b987554f-4214-409f-97a8-40d2b13c25ee",
    "outputId": "aac4f3c9-6f58-4c64-9985-f9a6162c0770"
   },
   "outputs": [],
   "source": [
    "print(str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed35466-06fc-4f09-a0c5-f4eda6d671a4",
   "metadata": {
    "id": "2ed35466-06fc-4f09-a0c5-f4eda6d671a4",
    "outputId": "91f62b81-67c7-4cf5-d8fa-bdd2f519439d"
   },
   "outputs": [],
   "source": [
    "# if you implemented this imperatively - you can still do so!\n",
    "# just make sure you format the prompt and call the right method on the LLM\n",
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "\n",
    "# try chaining basic prompts\n",
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "# format prompt, pass to LLM\n",
    "movie_name = \"The Departed\"\n",
    "full_prompt_tmpl = prompt_tmpl.format(movie_name=movie_name)\n",
    "response = llm.chat([ChatMessage(content=full_prompt_tmpl, role=MessageRole.USER)])\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44bac2-fff8-4578-8179-2cf68d075429",
   "metadata": {
    "id": "4f44bac2-fff8-4578-8179-2cf68d075429"
   },
   "source": [
    "### Try Output Parsing\n",
    "\n",
    "Let's parse the outputs into a structured Pydantic object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc9aa6-d74e-4d02-8101-83ee03d68d52",
   "metadata": {
    "id": "69fc9aa6-d74e-4d02-8101-83ee03d68d52"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"Object representing a single movie.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"Name of the movie.\")\n",
    "    year: int = Field(..., description=\"Year of the movie.\")\n",
    "\n",
    "\n",
    "class Movies(BaseModel):\n",
    "    \"\"\"Object representing a list of movies.\"\"\"\n",
    "\n",
    "    movies: List[Movie] = Field(..., description=\"List of movies.\")\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = PydanticOutputParser(Movies)\n",
    "json_prompt_str = \"\"\"\\\n",
    "Please generate related movies to {movie_name}.\n",
    "\"\"\"\n",
    "json_prompt_str = output_parser.format(json_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b337b-da7a-485d-8f16-13f6f71f4978",
   "metadata": {
    "id": "2c1b337b-da7a-485d-8f16-13f6f71f4978",
    "outputId": "267f8f44-68c5-4b73-f472-2740864cd103"
   },
   "outputs": [],
   "source": [
    "print(json_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b55ca-a7bd-43e4-a837-20e29b3bebed",
   "metadata": {
    "id": "243b55ca-a7bd-43e4-a837-20e29b3bebed",
    "outputId": "e5de188a-5a8d-44ed-9a18-180c7876454b"
   },
   "outputs": [],
   "source": [
    "# add JSON spec to prompt template\n",
    "json_prompt_tmpl = PromptTemplate(json_prompt_str)\n",
    "\n",
    "p = QueryPipeline(chain=[json_prompt_tmpl, llm, output_parser], verbose=True)\n",
    "output = p.run(movie_name=\"Toy Story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15bef7-399b-4dcc-94d6-6a4ea0066a41",
   "metadata": {
    "id": "4b15bef7-399b-4dcc-94d6-6a4ea0066a41",
    "outputId": "023939b5-ae1f-4750-f413-62cf280f8754"
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdfacf4-fb27-423a-b99c-37a84a1e2fbc",
   "metadata": {
    "id": "4fdfacf4-fb27-423a-b99c-37a84a1e2fbc"
   },
   "source": [
    "### Streaming Support\n",
    "\n",
    "The query pipelines have LLM streaming support (simply do `as_query_component(streaming=True)`). Intermediate outputs will get autoconverted, and the final output can be a streaming output. Here's some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92e2e2-9f66-47d3-9dbf-db6c7587e893",
   "metadata": {
    "id": "ab92e2e2-9f66-47d3-9dbf-db6c7587e893"
   },
   "source": [
    "**1. Chain multiple Prompts with Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba999ee-2281-46e5-9538-61297d73fe2e",
   "metadata": {
    "id": "4ba999ee-2281-46e5-9538-61297d73fe2e"
   },
   "outputs": [],
   "source": [
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "# let's add some subsequent prompts for fun\n",
    "prompt_str2 = \"\"\"\\\n",
    "Here's some text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Can you rewrite this with a summary of each movie?\n",
    "\"\"\"\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm_c = llm.as_query_component(streaming=True)\n",
    "\n",
    "p = QueryPipeline(\n",
    "    chain=[prompt_tmpl, llm_c, prompt_tmpl2, llm_c], verbose=True\n",
    ")\n",
    "# p = QueryPipeline(chain=[prompt_tmpl, llm_c], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00485c-b5d5-4504-80c0-c8614e51523d",
   "metadata": {
    "id": "0c00485c-b5d5-4504-80c0-c8614e51523d",
    "outputId": "52a70727-095a-4730-ac77-ea7dfb0e7d24"
   },
   "outputs": [],
   "source": [
    "output = p.run(movie_name=\"The Dark Knight\")\n",
    "for o in output:\n",
    "    print(o.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55168e-6d82-41e9-9e98-51025dc0ed3d",
   "metadata": {
    "id": "bf55168e-6d82-41e9-9e98-51025dc0ed3d"
   },
   "source": [
    "**2. Feed streaming output to output parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61752c36-8e7b-462a-ab23-4e3453d4b0b0",
   "metadata": {
    "id": "61752c36-8e7b-462a-ab23-4e3453d4b0b0",
    "outputId": "1e70e7d0-ba0d-4b46-aa8e-edadc6e77944"
   },
   "outputs": [],
   "source": [
    "p = QueryPipeline(\n",
    "    chain=[\n",
    "        json_prompt_tmpl,\n",
    "        llm.as_query_component(streaming=True),\n",
    "        output_parser,\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "output = p.run(movie_name=\"Toy Story\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572cdd4-2c94-4871-a496-623e9779e0db",
   "metadata": {
    "id": "4572cdd4-2c94-4871-a496-623e9779e0db"
   },
   "source": [
    "## Chain Together Query Rewriting Workflow (prompts + LLM) with Retrieval\n",
    "\n",
    "Here we try a slightly more complex workflow where we send the input through two prompts before initiating retrieval.\n",
    "\n",
    "1. Generate question about given topic.\n",
    "2. Hallucinate answer given question, for better retrieval.\n",
    "\n",
    "Since each prompt only takes in one input, note that the `QueryPipeline` will automatically chain LLM outputs into the prompt and then into the LLM.\n",
    "\n",
    "You'll see how to define links more explicitly in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6908b1-5819-4f34-a06c-2f8b9fc81c3e",
   "metadata": {
    "id": "3c6908b1-5819-4f34-a06c-2f8b9fc81c3e"
   },
   "outputs": [],
   "source": [
    "# from llama_index.postprocessor import CohereRerank\n",
    "\n",
    "# generate question regarding topic\n",
    "prompt_str1 = \"Please generate a concise question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "# use HyDE to hallucinate answer.\n",
    "prompt_str2 = (\n",
    "    \"Please write a passage to answer the question\\n\"\n",
    "    \"Try to include as many key details as possible.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    'Passage:\"\"\"\\n'\n",
    ")\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "p = QueryPipeline(\n",
    "    chain=[prompt_tmpl1, llm, prompt_tmpl2, llm, retriever], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb2534-a69a-46fd-b539-84ae93e2e5bb",
   "metadata": {
    "id": "03eb2534-a69a-46fd-b539-84ae93e2e5bb",
    "outputId": "a0e65a93-79c7-46c2-e123-05506f21233c"
   },
   "outputs": [],
   "source": [
    "nodes = p.run(topic=\"college\")\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c073634-55d4-4066-977f-fbc189089b95",
   "metadata": {
    "id": "2c073634-55d4-4066-977f-fbc189089b95"
   },
   "source": [
    "## Create a Full RAG Pipeline as a DAG\n",
    "\n",
    "Here we chain together a full RAG pipeline consisting of query rewriting, retrieval, reranking, and response synthesis.\n",
    "\n",
    "Here we can't use `chain` syntax because certain modules depend on multiple inputs (for instance, response synthesis expects both the retrieved nodes and the original question). Instead we'll construct a DAG explicitly, through `add_modules` and then `add_link`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66625e37-a0f2-45f4-afaa-e93948444e97",
   "metadata": {
    "id": "66625e37-a0f2-45f4-afaa-e93948444e97"
   },
   "source": [
    "### 1. RAG Pipeline with Query Rewriting\n",
    "\n",
    "We use an LLM to rewrite the query first before passing it to our downstream modules - retrieval/reranking/synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531677b6-0e6a-4002-9fdd-2096f1a83685",
   "metadata": {
    "id": "531677b6-0e6a-4002-9fdd-2096f1a83685"
   },
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import CohereRerank\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "# define modules\n",
    "prompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "reranker = CohereRerank()\n",
    "## NOTE: we are deprecating ServiceContext soon in v0.10 and letting you pass in `llm` directly.\n",
    "summarizer = TreeSummarize(\n",
    "    service_context=ServiceContext.from_defaults(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2bd1e-8c32-420b-b557-eb7dbe81718f",
   "metadata": {
    "id": "4ea2bd1e-8c32-420b-b557-eb7dbe81718f"
   },
   "outputs": [],
   "source": [
    "# define query pipeline\n",
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"llm\": llm,\n",
    "        \"prompt_tmpl\": prompt_tmpl,\n",
    "        \"retriever\": retriever,\n",
    "        \"summarizer\": summarizer,\n",
    "        \"reranker\": reranker,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e2af5-f7c2-4ab8-bb7c-76c369b63650",
   "metadata": {
    "id": "7e4e2af5-f7c2-4ab8-bb7c-76c369b63650"
   },
   "source": [
    "Next we draw links between modules with `add_link`. `add_link` takes in the source/destination module ids, and optionally the `source_key` and `dest_key`. Specify the `source_key` or `dest_key` if there are multiple outputs/inputs respectively.\n",
    "\n",
    "You can view the set of input/output keys for each module through `module.as_query_component().input_keys` and `module.as_query_component().output_keys`.\n",
    "\n",
    "Here we explicitly specify `dest_key` for the `reranker` and `summarizer` modules because they take in two inputs (query_str and nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cccaf-8887-48f7-96b6-1d5458987022",
   "metadata": {
    "id": "0f0cccaf-8887-48f7-96b6-1d5458987022",
    "outputId": "c42126d5-39bc-4da5-d6e5-ebb6db79f5e8"
   },
   "outputs": [],
   "source": [
    "p.add_link(\"prompt_tmpl\", \"llm\")\n",
    "p.add_link(\"llm\", \"retriever\")\n",
    "p.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\n",
    "p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n",
    "\n",
    "# look at summarizer input keys\n",
    "print(summarizer.as_query_component().input_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf69738-b409-408a-ac01-62bd4c2c2db4",
   "metadata": {
    "id": "1cf69738-b409-408a-ac01-62bd4c2c2db4"
   },
   "source": [
    "We use `networkx` to store the graph representation. This gives us an easy way to view the DAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef45868-ce07-4190-b12e-a70b287a491f",
   "metadata": {
    "id": "4ef45868-ce07-4190-b12e-a70b287a491f",
    "outputId": "37d8ac3e-e325-4ed9-e8e8-d0d93fc76f34"
   },
   "outputs": [],
   "source": [
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(p.dag)\n",
    "net.show(\"rag_dag.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ea13f-88e2-4393-8bd3-97fe436ffc0c",
   "metadata": {
    "id": "574ea13f-88e2-4393-8bd3-97fe436ffc0c",
    "outputId": "b7db5bea-63fc-4465-9eae-1a9156293f2b"
   },
   "outputs": [],
   "source": [
    "response = p.run(topic=\"YC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998375e4-a8b4-4229-b034-6280d61e69f2",
   "metadata": {
    "id": "998375e4-a8b4-4229-b034-6280d61e69f2",
    "outputId": "c4a82da7-b6d6-4733-9521-901fda244dbf"
   },
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d976a65-1061-4b4a-8fdd-5dcf4c1bece9",
   "metadata": {
    "id": "2d976a65-1061-4b4a-8fdd-5dcf4c1bece9"
   },
   "outputs": [],
   "source": [
    "# you can do async too\n",
    "response = await p.arun(topic=\"YC\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce083e06-7943-4bc9-b134-ce4b7aa11f6a",
   "metadata": {
    "id": "ce083e06-7943-4bc9-b134-ce4b7aa11f6a"
   },
   "source": [
    "### 2. RAG Pipeline without Query Rewriting\n",
    "\n",
    "Here we setup a RAG pipeline without the query rewriting step.\n",
    "\n",
    "Here we need a way to link the input query to both the retriever, reranker, and summarizer. We can do this by defining a special `InputComponent`, allowing us to link the inputs to multiple downstream modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab324158-1491-46a7-95fd-c80ecd08957f",
   "metadata": {
    "id": "ab324158-1491-46a7-95fd-c80ecd08957f"
   },
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import CohereRerank\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.query_pipeline import InputComponent\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "summarizer = TreeSummarize(\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    )\n",
    ")\n",
    "reranker = CohereRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cc32f-f155-4b72-8407-7618aa07622e",
   "metadata": {
    "id": "230cc32f-f155-4b72-8407-7618aa07622e"
   },
   "outputs": [],
   "source": [
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"input\": InputComponent(),\n",
    "        \"retriever\": retriever,\n",
    "        \"summarizer\": summarizer,\n",
    "    }\n",
    ")\n",
    "p.add_link(\"input\", \"retriever\")\n",
    "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"retriever\", \"summarizer\", dest_key=\"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64384a-6574-464a-a2a4-fa5f25091025",
   "metadata": {
    "id": "2a64384a-6574-464a-a2a4-fa5f25091025",
    "outputId": "3b3e0747-539c-4d17-ff9c-b1e1e16ff6ad"
   },
   "outputs": [],
   "source": [
    "output = p.run(input=\"what did the author do in YC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f261e6-d4f9-444e-904c-c253f97123d2",
   "metadata": {
    "id": "82f261e6-d4f9-444e-904c-c253f97123d2",
    "outputId": "11c3ad76-96aa-4023-dcaa-c07073682400"
   },
   "outputs": [],
   "source": [
    "print(str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c11f28-983d-4b3e-94a1-541c2804b989",
   "metadata": {
    "id": "76c11f28-983d-4b3e-94a1-541c2804b989"
   },
   "source": [
    "## Defining a Custom Component in a Query Pipeline\n",
    "\n",
    "You can easily define a custom component. Simply subclass a `QueryComponent`, implement validation/run functions + some helpers, and plug it in.\n",
    "\n",
    "Let's wrap the related movie generation prompt+LLM chain from the first example into a custom component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c470d37-a427-44a0-9b66-c83cbcac2545",
   "metadata": {
    "id": "6c470d37-a427-44a0-9b66-c83cbcac2545"
   },
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import (\n",
    "    CustomQueryComponent,\n",
    "    InputKeys,\n",
    "    OutputKeys,\n",
    ")\n",
    "from typing import Dict, Any\n",
    "from llama_index.llms.llm import BaseLLM\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class RelatedMovieComponent(CustomQueryComponent):\n",
    "    \"\"\"Related movie component.\"\"\"\n",
    "\n",
    "    llm: BaseLLM = Field(..., description=\"OpenAI LLM\")\n",
    "\n",
    "    def _validate_component_inputs(\n",
    "        self, input: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Validate component inputs during run_component.\"\"\"\n",
    "        # NOTE: this is OPTIONAL but we show you here how to do validation as an example\n",
    "        return input\n",
    "\n",
    "    @property\n",
    "    def _input_keys(self) -> set:\n",
    "        \"\"\"Input keys dict.\"\"\"\n",
    "        # NOTE: These are required inputs. If you have optional inputs please override\n",
    "        # `optional_input_keys_dict`\n",
    "        return {\"movie\"}\n",
    "\n",
    "    @property\n",
    "    def _output_keys(self) -> set:\n",
    "        return {\"output\"}\n",
    "\n",
    "    def _run_component(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Run the component.\"\"\"\n",
    "        # use QueryPipeline itself here for convenience\n",
    "        prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "        prompt_tmpl = PromptTemplate(prompt_str)\n",
    "        p = QueryPipeline(chain=[prompt_tmpl, llm])\n",
    "        return {\"output\": p.run(movie_name=kwargs[\"movie\"])}\n",
    "\n",
    "\n",
    "# from llama_index.query_pipeline import FunctionComponent\n",
    "\n",
    "# def foo(x: str) -> str:\n",
    "#     return x + \":hello\"\n",
    "\n",
    "# component = FunctionComponent(fn=foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc1a62-1e84-45c5-9f12-cac7778bd46f",
   "metadata": {
    "id": "69dc1a62-1e84-45c5-9f12-cac7778bd46f"
   },
   "source": [
    "Let's try the custom component out! We'll also add a step to convert the output to Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff382d-baf8-46e2-b4c0-477a07a41219",
   "metadata": {
    "id": "d8ff382d-baf8-46e2-b4c0-477a07a41219"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "component = RelatedMovieComponent(llm=llm)\n",
    "\n",
    "# let's add some subsequent prompts for fun\n",
    "prompt_str = \"\"\"\\\n",
    "Here's some text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Can you rewrite this in the voice of Shakespeare?\n",
    "\"\"\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "\n",
    "p = QueryPipeline(chain=[component, prompt_tmpl, llm], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6aa04-4443-4511-966d-c71cbd268efa",
   "metadata": {
    "id": "b7f6aa04-4443-4511-966d-c71cbd268efa",
    "outputId": "7374589c-f52f-4143-f825-9610bdeeb54f"
   },
   "outputs": [],
   "source": [
    "output = p.run(movie=\"Love Actually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ee679-e6a3-4e57-9566-00ef9f40b837",
   "metadata": {
    "id": "726ee679-e6a3-4e57-9566-00ef9f40b837",
    "outputId": "823e4720-9ec2-4660-d469-a3b884c76310"
   },
   "outputs": [],
   "source": [
    "print(str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ccda7-4f40-4b37-8eff-7e993d7cc377",
   "metadata": {
    "id": "ef8ccda7-4f40-4b37-8eff-7e993d7cc377"
   },
   "source": [
    "## Async / Parallel Execution\n",
    "\n",
    "Here we showcase our query pipeline with async + parallel execution.\n",
    "\n",
    "We do this by setting up a RAG pipeline that does the following:\n",
    "1. Send query to multiple RAG query engines.\n",
    "2. Combine results.\n",
    "\n",
    "In the process we'll also show some nice abstractions for joining results (e.g. our `ArgPackComponent()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a798d4-6c85-4ba6-85bb-a4459273a75f",
   "metadata": {
    "id": "13a798d4-6c85-4ba6-85bb-a4459273a75f"
   },
   "source": [
    "### Define Multiple Query Engines (One per Chunk Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9eec6-f473-4e75-811c-37e288b6eda4",
   "metadata": {
    "id": "70e9eec6-f473-4e75-811c-37e288b6eda4"
   },
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    ArgPackComponent,\n",
    ")\n",
    "from typing import Dict, Any, List, Optional\n",
    "from llama_index.llama_pack.base import BaseLlamaPack\n",
    "from llama_index.llms.llm import LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index import Document, VectorStoreIndex, ServiceContext\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index.schema import NodeWithScore, TextNode\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "chunk_sizes = [128, 256, 512, 1024]\n",
    "query_engines = {}\n",
    "for chunk_size in chunk_sizes:\n",
    "    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    nodes = splitter.get_nodes_from_documents(docs)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    query_engines[str(chunk_size)] = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c863550-a7a1-4ff0-a1c6-1b2d99308561",
   "metadata": {
    "id": "8c863550-a7a1-4ff0-a1c6-1b2d99308561"
   },
   "source": [
    "### Construct a Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92968c-6f10-49c7-96a6-4b7cec59a31b",
   "metadata": {
    "id": "6d92968c-6f10-49c7-96a6-4b7cec59a31b"
   },
   "outputs": [],
   "source": [
    "# construct query pipeline\n",
    "p = QueryPipeline(verbose=True)\n",
    "module_dict = {\n",
    "    **query_engines,\n",
    "    \"input\": InputComponent(),\n",
    "    \"summarizer\": TreeSummarize(),\n",
    "    \"join\": ArgPackComponent(\n",
    "        convert_fn=lambda x: NodeWithScore(node=TextNode(text=str(x)))\n",
    "    ),\n",
    "}\n",
    "p.add_modules(module_dict)\n",
    "# add links from input to query engine (id'ed by chunk_size)\n",
    "for chunk_size in chunk_sizes:\n",
    "    p.add_link(\"input\", str(chunk_size))\n",
    "    p.add_link(str(chunk_size), \"join\", dest_key=str(chunk_size))\n",
    "p.add_link(\"join\", \"summarizer\", dest_key=\"nodes\")\n",
    "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe3cea-a633-4c84-b30a-523b330c0b7a",
   "metadata": {
    "id": "ffbe3cea-a633-4c84-b30a-523b330c0b7a"
   },
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bdc53-daaa-4a62-a093-55759265edca",
   "metadata": {
    "id": "1b2bdc53-daaa-4a62-a093-55759265edca",
    "outputId": "26e4899f-2407-4bd4-fb28-e3b6ce404c03"
   },
   "outputs": [],
   "source": [
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(p.dag)\n",
    "net.show(\"rag_dag.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5f6ff-0421-47e2-828d-9d058935d4a4",
   "metadata": {
    "id": "35e5f6ff-0421-47e2-828d-9d058935d4a4"
   },
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b92c3-79c7-42d9-bd28-3148b1d9df79",
   "metadata": {
    "id": "0c4b92c3-79c7-42d9-bd28-3148b1d9df79",
    "outputId": "2172f2d9-00e4-479f-aba3-27603ca12548"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "response = await p.arun(input=\"What did the author do during his time in YC?\")\n",
    "print(str(response))\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20d119-8620-4a4e-af1b-8d7309c7da3e",
   "metadata": {
    "id": "ac20d119-8620-4a4e-af1b-8d7309c7da3e",
    "outputId": "bee5cdf3-8419-4e6e-d3a6-06dc999275f7"
   },
   "outputs": [],
   "source": [
    "# compare with sync method\n",
    "\n",
    "start_time = time.time()\n",
    "response = p.run(input=\"What did the author do during his time in YC?\")\n",
    "print(str(response))\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52046cf-1dbc-447d-aa7a-0598b9bce3a5",
   "metadata": {
    "id": "d52046cf-1dbc-447d-aa7a-0598b9bce3a5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
