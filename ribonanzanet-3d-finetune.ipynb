{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87793,"databundleVersionId":11228175,"sourceType":"competition"},{"sourceId":8318191,"sourceType":"datasetVersion","datasetId":4459124},{"sourceId":7639698,"sourceType":"datasetVersion","datasetId":4299272}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport random\nimport pickle\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:16.553977Z","iopub.execute_input":"2025-02-27T00:45:16.554394Z","iopub.status.idle":"2025-02-27T00:45:16.558565Z","shell.execute_reply.started":"2025-02-27T00:45:16.554372Z","shell.execute_reply":"2025-02-27T00:45:16.557484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#set seed for everything\ntorch.manual_seed(0)\nnp.random.seed(0)\nrandom.seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:16.560037Z","iopub.execute_input":"2025-02-27T00:45:16.560352Z","iopub.status.idle":"2025-02-27T00:45:16.585813Z","shell.execute_reply.started":"2025-02-27T00:45:16.560318Z","shell.execute_reply":"2025-02-27T00:45:16.58513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"config = {\n    \"seed\": 0,\n    \"cutoff_date\": \"2020-01-01\",\n    \"test_cutoff_date\": \"2022-05-01\",\n    \"max_len\": 384,\n    \"batch_size\": 1,\n    \"learning_rate\": 1e-4,\n    \"weight_decay\": 0.0,\n    \"mixed_precision\": \"bf16\",\n    \"model_config_path\": \"../working/configs/pairwise.yaml\",  # Adjust path as needed\n    \"epochs\": 10,\n    \"cos_epoch\": 5,\n    \"loss_power_scale\": 1.0,\n    \"max_cycles\": 1,\n    \"grad_clip\": 0.1,\n    \"gradient_accumulation_steps\": 1,\n    \"d_clamp\": 30,\n    \"max_len_filter\": 9999999,\n    \"min_len_filter\": 10, \n    \"structural_violation_epoch\": 50,\n    \"balance_weight\": False,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:16.587491Z","iopub.execute_input":"2025-02-27T00:45:16.587733Z","iopub.status.idle":"2025-02-27T00:45:16.593545Z","shell.execute_reply.started":"2025-02-27T00:45:16.587714Z","shell.execute_reply":"2025-02-27T00:45:16.592881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get data and do some data processing¶\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T00:35:07.639563Z","iopub.execute_input":"2025-02-27T00:35:07.63984Z","iopub.status.idle":"2025-02-27T00:35:07.643454Z","shell.execute_reply.started":"2025-02-27T00:35:07.639817Z","shell.execute_reply":"2025-02-27T00:35:07.64259Z"}}},{"cell_type":"code","source":"# Load data\n\ntrain_sequences=pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding/train_sequences.csv\")\ntrain_labels=pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding/train_labels.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:16.594633Z","iopub.execute_input":"2025-02-27T00:45:16.594926Z","iopub.status.idle":"2025-02-27T00:45:16.959432Z","shell.execute_reply.started":"2025-02-27T00:45:16.594906Z","shell.execute_reply":"2025-02-27T00:45:16.958788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_labels[\"pdb_id\"] = train_labels[\"ID\"].apply(lambda x: x.split(\"_\")[0]+'_'+x.split(\"_\")[1])\ntrain_labels[\"pdb_id\"] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:16.960255Z","iopub.execute_input":"2025-02-27T00:45:16.960504Z","iopub.status.idle":"2025-02-27T00:45:17.047085Z","shell.execute_reply.started":"2025-02-27T00:45:16.960473Z","shell.execute_reply":"2025-02-27T00:45:17.046224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"float('Nan')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:17.048005Z","iopub.execute_input":"2025-02-27T00:45:17.048314Z","iopub.status.idle":"2025-02-27T00:45:17.054066Z","shell.execute_reply.started":"2025-02-27T00:45:17.04828Z","shell.execute_reply":"2025-02-27T00:45:17.053323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_xyz=[]\n\nfor pdb_id in tqdm(train_sequences['target_id']):\n    df = train_labels[train_labels[\"pdb_id\"]==pdb_id]\n    #break\n    xyz=df[['x_1','y_1','z_1']].to_numpy().astype('float32')\n    xyz[xyz<-1e17]=float('Nan');\n    all_xyz.append(xyz)\n\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:17.054885Z","iopub.execute_input":"2025-02-27T00:45:17.055166Z","iopub.status.idle":"2025-02-27T00:45:25.429186Z","shell.execute_reply.started":"2025-02-27T00:45:17.055133Z","shell.execute_reply":"2025-02-27T00:45:25.428481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filter the data\n# Filter and process data\nfilter_nan = []\nmax_len = 0\nfor xyz in all_xyz:\n    if len(xyz) > max_len:\n        max_len = len(xyz)\n\n    #fill -1e18 masked sequences to nans\n    \n    #sugar_xyz = np.stack([nt_xyz['sugar_ring'] for nt_xyz in xyz], axis=0)\n    filter_nan.append((np.isnan(xyz).mean() <= 0.5) & \\\n                      (len(xyz)<config['max_len_filter']) & \\\n                      (len(xyz)>config['min_len_filter']))\n\nprint(f\"Longest sequence in train: {max_len}\")\n\nfilter_nan = np.array(filter_nan)\nnon_nan_indices = np.arange(len(filter_nan))[filter_nan]\n\ntrain_sequences = train_sequences.loc[non_nan_indices].reset_index(drop=True)\nall_xyz=[all_xyz[i] for i in non_nan_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.431131Z","iopub.execute_input":"2025-02-27T00:45:25.431369Z","iopub.status.idle":"2025-02-27T00:45:25.448995Z","shell.execute_reply.started":"2025-02-27T00:45:25.431349Z","shell.execute_reply":"2025-02-27T00:45:25.448103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pack data into a dictionary\n\ndata={\n      \"sequence\":train_sequences['sequence'].to_list(),\n      \"temporal_cutoff\": train_sequences['temporal_cutoff'].to_list(),\n      \"description\": train_sequences['description'].to_list(),\n      \"all_sequences\": train_sequences['all_sequences'].to_list(),\n      \"xyz\": all_xyz\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.450057Z","iopub.execute_input":"2025-02-27T00:45:25.45028Z","iopub.status.idle":"2025-02-27T00:45:25.454317Z","shell.execute_reply.started":"2025-02-27T00:45:25.450262Z","shell.execute_reply":"2025-02-27T00:45:25.453722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split train data into train/val/test¶\nWe will simply do a temporal split, because that's how testing is done in structural biology in general (in actual blind tests)","metadata":{}},{"cell_type":"code","source":"# Split data into train and test\nall_index = np.arange(len(data['sequence']))\ncutoff_date = pd.Timestamp(config['cutoff_date'])\ntest_cutoff_date = pd.Timestamp(config['test_cutoff_date'])\ntrain_index = [i for i, d in enumerate(data['temporal_cutoff']) if pd.Timestamp(d) <= cutoff_date]\ntest_index = [i for i, d in enumerate(data['temporal_cutoff']) if pd.Timestamp(d) > cutoff_date and pd.Timestamp(d) <= test_cutoff_date]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.455205Z","iopub.execute_input":"2025-02-27T00:45:25.455483Z","iopub.status.idle":"2025-02-27T00:45:25.474757Z","shell.execute_reply.started":"2025-02-27T00:45:25.455454Z","shell.execute_reply":"2025-02-27T00:45:25.474028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train size: {len(train_index)}\")\nprint(f\"Test size: {len(test_index)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.475574Z","iopub.execute_input":"2025-02-27T00:45:25.475942Z","iopub.status.idle":"2025-02-27T00:45:25.48995Z","shell.execute_reply.started":"2025-02-27T00:45:25.475902Z","shell.execute_reply":"2025-02-27T00:45:25.489302Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get pytorch dataset¶","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom ast import literal_eval\n\ndef get_ct(bp,s):\n    ct_matrix=np.zeros((len(s),len(s)))\n    for b in bp:\n        ct_matrix[b[0]-1,b[1]-1]=1\n    return ct_matrix\n\nclass RNA3D_Dataset(Dataset):\n    def __init__(self,indices,data):\n        self.indices=indices\n        self.data=data\n        self.tokens={nt:i for i,nt in enumerate('ACGU')}\n\n    def __len__(self):\n        return len(self.indices)\n    \n    def __getitem__(self, idx):\n\n        idx=self.indices[idx]\n        sequence=[self.tokens[nt] for nt in (self.data['sequence'][idx])]\n        sequence=np.array(sequence)\n        sequence=torch.tensor(sequence)\n\n        #get C1' xyz\n        xyz=self.data['xyz'][idx]\n        xyz=torch.tensor(np.array(xyz))\n\n\n        if len(sequence)>config['max_len']:\n            crop_start=np.random.randint(len(sequence)-config['max_len'])\n            crop_end=crop_start+config['max_len']\n\n            sequence=sequence[crop_start:crop_end]\n            xyz=xyz[crop_start:crop_end]\n        \n\n        return {'sequence':sequence,\n                'xyz':xyz}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.490698Z","iopub.execute_input":"2025-02-27T00:45:25.490918Z","iopub.status.idle":"2025-02-27T00:45:25.504781Z","shell.execute_reply.started":"2025-02-27T00:45:25.490895Z","shell.execute_reply":"2025-02-27T00:45:25.504111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset=RNA3D_Dataset(train_index,data)\nval_dataset=RNA3D_Dataset(test_index,data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.50545Z","iopub.execute_input":"2025-02-27T00:45:25.50568Z","iopub.status.idle":"2025-02-27T00:45:25.520155Z","shell.execute_reply.started":"2025-02-27T00:45:25.505661Z","shell.execute_reply":"2025-02-27T00:45:25.519363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.graph_objects as go\nimport numpy as np\n\n\n\n# Example: Generate an Nx3 matrix\nxyz = train_dataset[200]['xyz']  # Replace this with your actual Nx3 data\nN = len(xyz)\n\n\nfor _ in range(2): #plot twice because it doesnt show up on first try for some reason\n    # Extract columns\n    x, y, z = xyz[:, 0], xyz[:, 1], xyz[:, 2]\n    \n    # Create the 3D scatter plot\n    fig = go.Figure(data=[go.Scatter3d(\n        x=x, y=y, z=z,\n        mode='markers',\n        marker=dict(\n            size=5,\n            color=z,  # Coloring based on z-value\n            colorscale='Viridis',  # Choose a colorscale\n            opacity=0.8\n        )\n    )])\n    \n    # Customize layout\n    fig.update_layout(\n        scene=dict(\n            xaxis_title=\"X\",\n            yaxis_title=\"Y\",\n            zaxis_title=\"Z\"\n        ),\n        title=\"3D Scatter Plot\"\n    )\n\nfig.show()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.520955Z","iopub.execute_input":"2025-02-27T00:45:25.521205Z","iopub.status.idle":"2025-02-27T00:45:25.764557Z","shell.execute_reply.started":"2025-02-27T00:45:25.52118Z","shell.execute_reply":"2025-02-27T00:45:25.763789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader=DataLoader(train_dataset,batch_size=1,shuffle=True)\nval_loader=DataLoader(val_dataset,batch_size=1,shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.765418Z","iopub.execute_input":"2025-02-27T00:45:25.765753Z","iopub.status.idle":"2025-02-27T00:45:25.770179Z","shell.execute_reply.started":"2025-02-27T00:45:25.765723Z","shell.execute_reply":"2025-02-27T00:45:25.769368Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get RibonanzaNet¶\nWe will add a linear layer to predict xyz of C1' atoms","metadata":{}},{"cell_type":"code","source":"! pip install einops\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:25.771063Z","iopub.execute_input":"2025-02-27T00:45:25.771302Z","iopub.status.idle":"2025-02-27T00:45:29.904099Z","shell.execute_reply.started":"2025-02-27T00:45:25.771275Z","shell.execute_reply":"2025-02-27T00:45:29.903007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n\nsys.path.append(\"/kaggle/input/ribonanzanet2d-final\")\n\n\nfrom Network import *\nimport yaml\n\n\n\nclass Config:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n        self.entries=entries\n\n    def print(self):\n        print(self.entries)\n\ndef load_config_from_yaml(file_path):\n    with open(file_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return Config(**config)\n\n\n\nclass finetuned_RibonanzaNet(RibonanzaNet):\n    def __init__(self, config, pretrained=False):\n        config.dropout=0.1\n        super(finetuned_RibonanzaNet, self).__init__(config)\n        if pretrained:\n            self.load_state_dict(torch.load(\"/kaggle/input/ribonanzanet-weights/RibonanzaNet.pt\",map_location='cpu'))\n        # self.ct_predictor=nn.Sequential(nn.Linear(64,256),\n        #                                 nn.ReLU(),\n        #                                 nn.Linear(256,64),\n        #                                 nn.ReLU(),\n        #                                 nn.Linear(64,1)) \n        self.dropout=nn.Dropout(0.0)\n        self.xyz_predictor=nn.Linear(256,3)\n\n\n    \n    def forward(self,src):\n        \n        #with torch.no_grad():\n        sequence_features, pairwise_features=self.get_embeddings(src, torch.ones_like(src).long().to(src.device))\n\n\n        xyz=self.xyz_predictor(sequence_features)\n\n        return xyz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:29.905337Z","iopub.execute_input":"2025-02-27T00:45:29.905708Z","iopub.status.idle":"2025-02-27T00:45:29.913239Z","shell.execute_reply.started":"2025-02-27T00:45:29.905666Z","shell.execute_reply":"2025-02-27T00:45:29.91239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model=finetuned_RibonanzaNet(load_config_from_yaml(\"/kaggle/input/ribonanzanet2d-final/configs/pairwise.yaml\"),pretrained=True).cuda()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:31.497985Z","iopub.execute_input":"2025-02-27T00:45:31.498292Z","iopub.status.idle":"2025-02-27T00:45:31.716488Z","shell.execute_reply.started":"2025-02-27T00:45:31.498261Z","shell.execute_reply":"2025-02-27T00:45:31.715755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training loop¶\nwe will use dRMSD loss on the predicted xyz. the loss function is invariant to translations, rotations, and reflections. because dRMSD is invariant to reflections, it cannot distinguish chiral structures, so there may be better loss functions","metadata":{}},{"cell_type":"code","source":"def calculate_distance_matrix(X,Y,epsilon=1e-4):\n    return (torch.square(X[:,None]-Y[None,:])+epsilon).sum(-1).sqrt()\n\n\ndef dRMSD(pred_x,\n          pred_y,\n          gt_x,\n          gt_y,\n          epsilon=1e-4,Z=10,d_clamp=None):\n    pred_dm=calculate_distance_matrix(pred_x,pred_y)\n    gt_dm=calculate_distance_matrix(gt_x,gt_y)\n\n\n\n    mask=~torch.isnan(gt_dm)\n    mask[torch.eye(mask.shape[0]).bool()]=False\n\n    if d_clamp is not None:\n        rmsd=(torch.square(pred_dm[mask]-gt_dm[mask])+epsilon).clip(0,d_clamp**2)\n    else:\n        rmsd=torch.square(pred_dm[mask]-gt_dm[mask])+epsilon\n\n    return rmsd.sqrt().mean()/Z\n\ndef local_dRMSD(pred_x,\n          pred_y,\n          gt_x,\n          gt_y,\n          epsilon=1e-4,Z=10,d_clamp=30):\n    pred_dm=calculate_distance_matrix(pred_x,pred_y)\n    gt_dm=calculate_distance_matrix(gt_x,gt_y)\n\n\n\n    mask=(~torch.isnan(gt_dm))*(gt_dm<d_clamp)\n    mask[torch.eye(mask.shape[0]).bool()]=False\n\n\n\n    rmsd=torch.square(pred_dm[mask]-gt_dm[mask])+epsilon\n    # rmsd=(torch.square(pred_dm[mask]-gt_dm[mask])+epsilon).sqrt()/Z\n    #rmsd=torch.abs(pred_dm[mask]-gt_dm[mask])/Z\n    return rmsd.sqrt().mean()/Z\n\ndef dRMAE(pred_x,\n          pred_y,\n          gt_x,\n          gt_y,\n          epsilon=1e-4,Z=10,d_clamp=None):\n    pred_dm=calculate_distance_matrix(pred_x,pred_y)\n    gt_dm=calculate_distance_matrix(gt_x,gt_y)\n\n\n\n    mask=~torch.isnan(gt_dm)\n    mask[torch.eye(mask.shape[0]).bool()]=False\n\n    rmsd=torch.abs(pred_dm[mask]-gt_dm[mask])\n\n    return rmsd.mean()/Z\n\nimport torch\n\ndef align_svd_mae(input, target, Z=10):\n    \"\"\"\n    Aligns the input (Nx3) to target (Nx3) using SVD-based Procrustes alignment\n    and computes RMSD loss.\n    \n    Args:\n        input (torch.Tensor): Nx3 tensor representing the input points.\n        target (torch.Tensor): Nx3 tensor representing the target points.\n    \n    Returns:\n        aligned_input (torch.Tensor): Nx3 aligned input.\n        rmsd_loss (torch.Tensor): RMSD loss.\n    \"\"\"\n    assert input.shape == target.shape, \"Input and target must have the same shape\"\n\n    #mask \n    mask=~torch.isnan(target.sum(-1))\n\n    input=input[mask]\n    target=target[mask]\n    \n    # Compute centroids\n    centroid_input = input.mean(dim=0, keepdim=True)\n    centroid_target = target.mean(dim=0, keepdim=True)\n\n    # Center the points\n    input_centered = input - centroid_input.detach()\n    target_centered = target - centroid_target\n\n    # Compute covariance matrix\n    cov_matrix = input_centered.T @ target_centered\n\n    # SVD to find optimal rotation\n    U, S, Vt = torch.svd(cov_matrix)\n\n    # Compute rotation matrix\n    R = Vt @ U.T\n\n    # Ensure a proper rotation (det(R) = 1, no reflection)\n    if torch.det(R) < 0:\n        Vt[-1, :] *= -1\n        R = Vt @ U.T\n\n    # Rotate input\n    aligned_input = (input_centered @ R.T.detach()) + centroid_target.detach()\n\n    # # Compute RMSD loss\n    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n\n    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n    \n    # return aligned_input, rmsd_loss\n    return torch.abs(aligned_input-target).mean()/Z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:45:55.509601Z","iopub.execute_input":"2025-02-27T00:45:55.509959Z","iopub.status.idle":"2025-02-27T00:45:55.521025Z","shell.execute_reply.started":"2025-02-27T00:45:55.509932Z","shell.execute_reply":"2025-02-27T00:45:55.520054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.amp import GradScaler\n\nepochs=50\ncos_epoch=35\n\n\nbest_loss=np.inf\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=0.0, lr=0.0001) #no weight decay following AF\n\nbatch_size=1\n\n#for cycle in range(2):\n\ncriterion=torch.nn.BCEWithLogitsLoss(reduction='none')\n\nscaler = GradScaler()\n\nschedule=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(epochs-cos_epoch)*len(train_loader)//batch_size)\n\nbest_val_loss=99999999999\nfor epoch in range(epochs):\n    model.train()\n    tbar=tqdm(train_loader)\n    total_loss=0\n    oom=0\n    for idx, batch in enumerate(tbar):\n        #try:\n        sequence=batch['sequence'].cuda()\n        gt_xyz=batch['xyz'].cuda().squeeze()\n\n        #with torch.autocast(device_type='cuda', dtype=torch.float16):\n        pred_xyz=model(sequence).squeeze()\n        \n        loss=dRMAE(pred_xyz,pred_xyz,gt_xyz,gt_xyz) + align_svd_mae(pred_xyz, gt_xyz)\n             #local_dRMSD(pred_xyz,pred_xyz,gt_xyz,gt_xyz)\n\n        if loss!=loss:\n            stop\n\n        \n        (loss/batch_size).backward()\n\n        if (idx+1)%batch_size==0 or idx+1 == len(tbar):\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n            optimizer.step()\n            optimizer.zero_grad()\n            # scaler.scale(loss/batch_size).backward()\n            # scaler.unscale_(optimizer)\n            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n            # scaler.step(optimizer)\n            # scaler.update()\n\n            \n            if (epoch+1)>cos_epoch:\n                schedule.step()\n        #schedule.step()\n        total_loss+=loss.item()\n        \n        tbar.set_description(f\"Epoch {epoch + 1} Loss: {total_loss/(idx+1)} OOMs: {oom}\")\n\n\n\n        # except Exception:\n        #     #print(Exception)\n        #     oom+=1\n    tbar=tqdm(val_loader)\n    model.eval()\n    val_preds=[]\n    val_loss=0\n    for idx, batch in enumerate(tbar):\n        sequence=batch['sequence'].cuda()\n        gt_xyz=batch['xyz'].cuda().squeeze()\n\n        with torch.no_grad():\n            pred_xyz=model(sequence).squeeze()\n            loss=dRMAE(pred_xyz,pred_xyz,gt_xyz,gt_xyz)\n            \n        val_loss+=loss.item()\n        val_preds.append([gt_xyz.cpu().numpy(),pred_xyz.cpu().numpy()])\n    val_loss=val_loss/len(tbar)\n    print(f\"val loss: {val_loss}\")\n    \n    \n    \n    if val_loss<best_val_loss:\n        best_val_loss=val_loss\n        best_preds=val_preds\n        torch.save(model.state_dict(),'RibonanzaNet-3D.pt')\n\n    # 1.053595052265986 train loss after epoch 0\ntorch.save(model.state_dict(),'RibonanzaNet-3D-final.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:46:21.669341Z","iopub.execute_input":"2025-02-27T00:46:21.669662Z"}},"outputs":[],"execution_count":null}]}