{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87793,"databundleVersionId":11553390,"sourceType":"competition"},{"sourceId":11068753,"sourceType":"datasetVersion","datasetId":6896780}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nfrom pathlib import Path\n\n# Assuming you've mounted your dataset containing the wheels and Protenix code\nDATASET_PATH = '/kaggle/input/required-presets'\n\n# Install dependencies from wheels\nwheel_path = os.path.join(DATASET_PATH, 'wheels')\n!pip install -qqq --no-index --find-links {wheel_path} torch numpy pandas scipy rdkit protenix\n\n# Add Protenix to Python path\nprotenix_path = os.path.join(DATASET_PATH, 'Protenix')\nsys.path.append(protenix_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:33:28.721573Z","iopub.execute_input":"2025-04-04T13:33:28.721887Z","iopub.status.idle":"2025-04-04T13:36:06.097096Z","shell.execute_reply.started":"2025-04-04T13:33:28.72186Z","shell.execute_reply":"2025-04-04T13:36:06.096048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd \nvalidation_sequence = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv')\nvalidation_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/validation_labels.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:36:06.098228Z","iopub.execute_input":"2025-04-04T13:36:06.098484Z","iopub.status.idle":"2025-04-04T13:36:06.753676Z","shell.execute_reply.started":"2025-04-04T13:36:06.09846Z","shell.execute_reply":"2025-04-04T13:36:06.752771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_sequences = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_sequences.csv') \ntest_sequences.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:36:06.755385Z","iopub.execute_input":"2025-04-04T13:36:06.755709Z","iopub.status.idle":"2025-04-04T13:36:06.830671Z","shell.execute_reply.started":"2025-04-04T13:36:06.755678Z","shell.execute_reply":"2025-04-04T13:36:06.829989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:09:42.991203Z","iopub.execute_input":"2025-04-04T13:09:42.991506Z","iopub.status.idle":"2025-04-04T13:09:43.009622Z","shell.execute_reply.started":"2025-04-04T13:09:42.991479Z","shell.execute_reply":"2025-04-04T13:09:43.009034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:09:45.401912Z","iopub.execute_input":"2025-04-04T13:09:45.402189Z","iopub.status.idle":"2025-04-04T13:09:45.420398Z","shell.execute_reply.started":"2025-04-04T13:09:45.402169Z","shell.execute_reply":"2025-04-04T13:09:45.419538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:10:05.712892Z","iopub.execute_input":"2025-04-04T13:10:05.713183Z","iopub.status.idle":"2025-04-04T13:10:05.71803Z","shell.execute_reply.started":"2025-04-04T13:10:05.713161Z","shell.execute_reply":"2025-04-04T13:10:05.717163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nRNA 3D Structure Prediction and Submission Generator for Kaggle Environment\n\nThis script:\n1. Reads RNA sequences from test_sequences.csv\n2. Creates input JSONs for each sequence\n3. Runs the Protenix model to predict 3D structures\n4. Extracts C1' atom coordinates from the output CIF files\n5. Creates a submission.csv file in the format required\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nimport pandas as pd\nimport numpy as np\nimport glob\nfrom tqdm import tqdm\nfrom biotite.structure.io import pdbx\n\ndef create_input_json(sequence, target_id):\n    \"\"\"\n    Create the input JSON for a single RNA sequence\n    \"\"\"\n    input_json = [{\n        \"sequences\": [\n            {\n                \"rnaSequence\": {\n                    \"sequence\": sequence,\n                    \"count\": 1,\n                    \"modifications\": []\n                }\n            }\n        ],\n        \"name\": target_id,\n        \"covalent_bonds\": []\n    }]\n    return input_json\n\ndef run_inference(input_json_path, output_dir, target_id):\n    \"\"\"\n    Run inference using the Protenix model with kaggle-specific paths\n    \"\"\"\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define command with kaggle-specific paths\n    cmd = [\n        \"python\", \"/kaggle/input/required-presets/Protenix/runner/inference.py\",\n        \"--seeds\", \"42\",\n        \"--dump_dir\", output_dir,\n        \"--input_json_path\", input_json_path,\n        \"--model.N_cycle\", \"10\",\n        \"--sample_diffusion.N_sample\", \"5\",\n        \"--sample_diffusion.N_step\", \"200\",\n        \"--load_checkpoint_path\", \"/kaggle/input/required-presets/Protenix/release_data/checkpoint/model_v0.2.0.pt\",\n        \"--use_deepspeed_evo_attention\", \"false\"\n    ]\n    \n    print(f\"Running inference for {target_id}\")\n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(f\"Error running inference for {target_id}: {result.stderr}\")\n            return False\n        return True\n    except Exception as e:\n        print(f\"Exception running inference for {target_id}: {e}\")\n        return False\n\ndef extract_c1_coordinates(cif_file_path):\n    \"\"\"\n    Extract C1' atom coordinates from a CIF file using biotite\n    \"\"\"\n    try:\n        # Read the CIF file using the correct biotite method\n        with open(cif_file_path, 'r') as f:\n            cif_data = pdbx.CIFFile.read(f)\n        \n        # Get structure from CIF data\n        atom_array = pdbx.get_structure(cif_data, model=1)\n        \n        # Clean atom names and find C1' atoms\n        atom_names_clean = np.char.strip(atom_array.atom_name.astype(str))\n        mask_c1 = atom_names_clean == \"C1'\"\n        c1_atoms = atom_array[mask_c1]\n        \n        if len(c1_atoms) == 0:\n            print(f\"Warning: No C1' atoms found in {cif_file_path}\")\n            return None\n        \n        # Sort by residue ID and return coordinates\n        sort_indices = np.argsort(c1_atoms.res_id)\n        c1_atoms_sorted = c1_atoms[sort_indices]\n        c1_coords = c1_atoms_sorted.coord\n        \n        return c1_coords\n    except Exception as e:\n        print(f\"Error extracting C1' coordinates from {cif_file_path}: {e}\")\n        return None\n\n\ndef process_sequence(sequence, target_id, temp_dir, output_dir):\n    \"\"\"\n    Process a single RNA sequence and return C1' coordinates\n    \"\"\"\n    print(f\"Processing {target_id}: {sequence}\")\n    \n    # Create input JSON\n    input_json = create_input_json(sequence, target_id)\n    \n    # Save JSON to temporary file\n    os.makedirs(temp_dir, exist_ok=True)\n    input_json_path = os.path.join(temp_dir, f\"{target_id}_input.json\")\n    with open(input_json_path, \"w\") as f:\n        json.dump(input_json, f, indent=4)\n    \n    # Run inference\n    success = run_inference(input_json_path, output_dir, target_id)\n    \n    if not success:\n        print(f\"Inference failed for {target_id}\")\n        return None\n    \n    # Find the CIF files for this target\n    target_prediction_dir = os.path.join(output_dir, target_id, \"seed_42\", \"predictions\")\n    if not os.path.exists(target_prediction_dir):\n        print(f\"Prediction directory not found for {target_id}\")\n        return None\n    \n    # Look for CIF files with the pattern {target_id}_seed_42_sample_*.cif\n    cif_files = sorted(glob.glob(os.path.join(target_prediction_dir, f\"{target_id}_seed_42_sample_*.cif\")))\n    \n    # If no CIF files found, return None\n    if not cif_files:\n        print(f\"No CIF files found for {target_id}\")\n        return None\n    \n    print(f\"Found {len(cif_files)} CIF files for {target_id}\")\n    \n    # Extract C1' coordinates from each CIF file\n    all_coords = []\n    for cif_file in cif_files:\n        coords = extract_c1_coordinates(cif_file)\n        if coords is not None:\n            all_coords.append(coords)\n    \n    if not all_coords:\n        print(f\"No valid C1' coordinates found for {target_id}\")\n        return None\n    \n    # Ensure we have 5 models (if we have fewer, duplicate the last one)\n    while len(all_coords) < 5:\n        print(f\"Only {len(all_coords)} models found for {target_id}, duplicating last model\")\n        all_coords.append(all_coords[-1])\n    \n    return all_coords[:5]  # Ensure we only have 5 models\n\ndef create_submission(test_sequences_df, c1_coords_dict, output_file):\n    \"\"\"\n    Create the submission CSV file with C1' coordinates\n    \"\"\"\n    rows = []\n    \n    # Process each sequence\n    for _, row in test_sequences_df.iterrows():\n        target_id = row['target_id']\n        sequence = row['sequence']\n        \n        if target_id not in c1_coords_dict or c1_coords_dict[target_id] is None:\n            print(f\"No prediction found for {target_id}, using zeros\")\n            # Create empty predictions (all zeros)\n            for i, residue in enumerate(sequence):\n                row_data = {\n                    'ID': f\"{target_id}_{i+1}\",\n                    'resname': residue,\n                    'resid': i+1\n                }\n                for model in range(1, 6):\n                    row_data[f'x_{model}'] = 0.0\n                    row_data[f'y_{model}'] = 0.0\n                    row_data[f'z_{model}'] = 0.0\n                rows.append(row_data)\n        else:\n            # Get the 5 models for this target\n            models = c1_coords_dict[target_id]\n            \n            # Create a row for each residue\n            for i, residue in enumerate(sequence):\n                row_data = {\n                    'ID': f\"{target_id}_{i+1}\",\n                    'resname': residue,\n                    'resid': i+1\n                }\n                \n                # Add coordinates for each model\n                for model_idx in range(5):\n                    if model_idx < len(models) and i < len(models[model_idx]):\n                        row_data[f'x_{model_idx+1}'] = models[model_idx][i][0]\n                        row_data[f'y_{model_idx+1}'] = models[model_idx][i][1]\n                        row_data[f'z_{model_idx+1}'] = models[model_idx][i][2]\n                    else:\n                        # If coordinates are not available, use zeros\n                        row_data[f'x_{model_idx+1}'] = 0.0\n                        row_data[f'y_{model_idx+1}'] = 0.0\n                        row_data[f'z_{model_idx+1}'] = 0.0\n                \n                rows.append(row_data)\n    \n    # Create DataFrame and save to CSV\n    df = pd.DataFrame(rows)\n    df.to_csv(output_file, index=False)\n    print(f\"Created submission file: {output_file}\")\n\ndef main():\n    \"\"\"\n    Main function\n    \"\"\"\n    # Set up required symlinks for CCD cache as in kaggle_inference.py\n    os.makedirs(\"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache\", exist_ok=True)\n    \n    source_ccd_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif\"\n    target_ccd_file = \"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache/components.v20240608.cif\"\n    \n    source_rdkit_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n    target_rdkit_file = \"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n    \n    # Create the symlinks if the source files exist\n    if os.path.exists(source_ccd_file) and not os.path.exists(target_ccd_file):\n        try:\n            os.symlink(source_ccd_file, target_ccd_file)\n            print(f\"Created symlink for CCD file\")\n        except Exception as e:\n            print(f\"Error creating symlink for CCD file: {e}\")\n    \n    if os.path.exists(source_rdkit_file) and not os.path.exists(target_rdkit_file):\n        try:\n            os.symlink(source_rdkit_file, target_rdkit_file)\n            print(f\"Created symlink for RDKIT file\")\n        except Exception as e:\n            print(f\"Error creating symlink for RDKIT file: {e}\")\n    \n    # Create directories\n    temp_dir = \"./input\"  # Same as in kaggle_inference.py\n    output_dir = \"./output\"  # Same as in kaggle_inference.py\n    os.makedirs(temp_dir, exist_ok=True)\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Read test sequences\n    test_sequences_df = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\")\n    print(f\"Loaded {len(test_sequences_df)} test sequences\")\n    \n    # Process each sequence\n    c1_coords_dict = {}\n    for _, row in tqdm(test_sequences_df.iterrows(), total=len(test_sequences_df)):\n        target_id = row['target_id']\n        sequence = row['sequence']\n        \n        # Check if we already have predictions for this target\n        target_prediction_dir = os.path.join(output_dir, target_id, \"seed_42\", \"predictions\")\n        if os.path.exists(target_prediction_dir):\n            print(f\"Found existing prediction for {target_id}, loading coordinates\")\n            # Extract coordinates from existing predictions\n            cif_files = sorted(glob.glob(os.path.join(target_prediction_dir, f\"{target_id}_seed_42_sample_*.cif\")))\n            \n            all_coords = []\n            for cif_file in cif_files:\n                coords = extract_c1_coordinates(cif_file)\n                if coords is not None:\n                    all_coords.append(coords)\n            \n            if all_coords:\n                # Ensure we have 5 models\n                while len(all_coords) < 5:\n                    all_coords.append(all_coords[-1])\n                c1_coords_dict[target_id] = all_coords[:5]\n                continue\n        \n        # Process the sequence if no existing prediction was found or was invalid\n        c1_coords = process_sequence(sequence, target_id, temp_dir, output_dir)\n        c1_coords_dict[target_id] = c1_coords\n    \n    # Create submission file\n    create_submission(test_sequences_df, c1_coords_dict, \"submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:43:34.995568Z","iopub.execute_input":"2025-04-04T13:43:34.99588Z","iopub.status.idle":"2025-04-04T14:21:40.421062Z","shell.execute_reply.started":"2025-04-04T13:43:34.995853Z","shell.execute_reply":"2025-04-04T14:21:40.420066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:25:33.391803Z","iopub.execute_input":"2025-04-04T14:25:33.392158Z","iopub.status.idle":"2025-04-04T14:25:33.424164Z","shell.execute_reply.started":"2025-04-04T14:25:33.392131Z","shell.execute_reply":"2025-04-04T14:25:33.423395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # Check if the components.cif file exists\n# ccd_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif\"\n# rdkit_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n\n# print(f\"CCD file exists: {os.path.exists(ccd_file)}\")\n# print(f\"RDKIT file exists: {os.path.exists(rdkit_file)}\")\n\n# # If they don't exist, let's look for them\n# if not (os.path.exists(ccd_file) and os.path.exists(rdkit_file)):\n#     print(\"Searching for CCD files in /kaggle/input/required-presets/Protenix/release_data/...\")\n#     for root, dirs, files in os.walk(\"/kaggle/input/required-presets/Protenix/release_data/\"):\n#         for file in files:\n#             if \"components\" in file and (\"cif\" in file or \"rdkit\" in file):\n#                 print(f\"Found: {os.path.join(root, file)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:39:03.501642Z","iopub.status.idle":"2025-04-04T13:39:03.501895Z","shell.execute_reply":"2025-04-04T13:39:03.501788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import subprocess\n\n# # Create the directory structure needed\n# os.makedirs(\"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache\", exist_ok=True)\n\n# # Create symlinks to the actual files\n# source_ccd_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif\"\n# target_ccd_file = \"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache/components.v20240608.cif\"\n\n# source_rdkit_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n# target_rdkit_file = \"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n\n# # Check if the source files exist\n# print(f\"Source CCD file exists: {os.path.exists(source_ccd_file)}\")\n# print(f\"Source RDKIT file exists: {os.path.exists(source_rdkit_file)}\")\n\n# # Create the symlinks if the source files exist\n# if os.path.exists(source_ccd_file):\n#     try:\n#         os.symlink(source_ccd_file, target_ccd_file)\n#         print(f\"Created symlink for CCD file\")\n#     except FileExistsError:\n#         print(f\"Symlink for CCD file already exists\")\n#     except Exception as e:\n#         print(f\"Error creating symlink for CCD file: {e}\")\n# else:\n#     print(f\"Cannot create symlink, source CCD file doesn't exist\")\n\n# if os.path.exists(source_rdkit_file):\n#     try:\n#         os.symlink(source_rdkit_file, target_rdkit_file)\n#         print(f\"Created symlink for RDKIT file\")\n#     except FileExistsError:\n#         print(f\"Symlink for RDKIT file already exists\")\n#     except Exception as e:\n#         print(f\"Error creating symlink for RDKIT file: {e}\")\n# else:\n#     print(f\"Cannot create symlink, source RDKIT file doesn't exist\")\n\n# # Create RNA input JSON\n# input_json = [{\n#     \"sequences\": [\n#         {\n#             \"rnaSequence\": {\n#                 \"sequence\": \"GGGUGCUCAGUACGAGAGGAACCGCACCC\",\n#                 \"count\": 1,\n#                 \"modifications\": []\n#             }\n#         }\n#     ],\n#     \"name\": \"rna_prediction\",\n#     \"covalent_bonds\": []\n# }]\n\n# # Save input JSON\n# os.makedirs(\"./input\", exist_ok=True)\n# with open(\"./input/rna_input.json\", \"w\") as f:\n#     json.dump(input_json, f, indent=4)\n\n# # Run inference using subprocess\n# cmd = [\n#     \"python\", \"/kaggle/input/required-presets/Protenix/runner/inference.py\",\n#     \"--seeds\", \"42\",\n#     \"--dump_dir\", \"./output\",\n#     \"--input_json_path\", \"./input/rna_input.json\",\n#     \"--model.N_cycle\", \"10\",\n#     \"--sample_diffusion.N_sample\", \"5\",\n#     \"--sample_diffusion.N_step\", \"200\",\n#     \"--load_checkpoint_path\", \"/kaggle/input/required-presets/Protenix/release_data/checkpoint/model_v0.2.0.pt\",\n#     \"--use_deepspeed_evo_attention\", \"false\"\n# ]\n\n# # Run the command\n# result = subprocess.run(cmd, capture_output=True, text=True)\n# print(\"STDOUT:\", result.stdout)\n# print(\"STDERR:\", result.stderr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:39:03.50271Z","iopub.status.idle":"2025-04-04T13:39:03.50311Z","shell.execute_reply":"2025-04-04T13:39:03.50295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check for error files\n# if os.path.exists(\"./output/ERR\"):\n#     print(\"Error directory exists!\")\n#     print(\"Contents:\")\n#     for item in os.listdir(\"./output/ERR\"):\n#         print(f\" - {item}\")\n    \n#     # If there are error files, show their contents\n#     error_files = os.listdir(\"./output/ERR\")\n#     if error_files:\n#         with open(os.path.join(\"./output/ERR\", error_files[0]), \"r\") as f:\n#             print(f\"Contents of {error_files[0]}:\")\n#             print(f.read())\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:43:02.500381Z","iopub.execute_input":"2025-04-04T10:43:02.500601Z","iopub.status.idle":"2025-04-04T10:43:02.506377Z","shell.execute_reply.started":"2025-04-04T10:43:02.500582Z","shell.execute_reply":"2025-04-04T10:43:02.505427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import pandas as pd\n# import biotite.structure.io.pdbx as pdbx\n# import matplotlib.pyplot as plt\n# from mpl_toolkits.mplot3d import Axes3D\n# import numpy as np\n\n# # List all the prediction files\n# output_dir = \"/kaggle/working/output/rna_prediction/seed_42/predictions/\"\n# cif_files = [f for f in os.listdir(output_dir) if f.endswith(\".cif\")]\n# print(f\"Found {len(cif_files)} prediction files:\")\n# for file in cif_files:\n#     print(f\" - {file}\")\n\n# # Read and analyze the first prediction file\n# if cif_files:\n#     # Read CIF file\n#     cif_file = os.path.join(output_dir, cif_files[0])\n#     with open(cif_file, 'r') as f:\n#         cif_data = pdbx.CIFFile.read(f)\n\n#     atom_array = pdbx.get_structure(cif_data, model=1)\n    \n#     print(f\"\\nStructure information for {cif_files[0]}:\")\n#     print(f\"Number of atoms: {len(atom_array)}\")\n#     print(f\"Residue count: {len(np.unique(atom_array.res_id))}\")\n\n#     # Clean and extract C1' atoms\n#     atom_names_clean = np.char.strip(atom_array.atom_name.astype(str))\n#     mask_c1 = atom_names_clean == \"C1'\"\n#     c1_atoms = atom_array[mask_c1]\n#     c1_coords = c1_atoms.coord\n\n#     print(f\"\\nFound {len(c1_atoms)} C1' atoms\")\n\n#     # Create DataFrame\n#     df = pd.DataFrame({\n#         \"res_name\": c1_atoms.res_name,\n#         \"res_id\": c1_atoms.res_id,\n#         \"chain_id\": c1_atoms.chain_id,\n#         \"x\": c1_coords[:, 0],\n#         \"y\": c1_coords[:, 1],\n#         \"z\": c1_coords[:, 2]\n#     })\n\n#     print(\"\\nFirst few C1' atoms:\")\n#     print(df.head())\n\n#     # Save to CSV and JSON\n#     df.to_csv(\"c1_prime_coordinates.csv\", index=False)\n#     df.to_json(\"c1_prime_coordinates.json\", orient=\"records\", indent=2)\n#     print(\"Saved C1' coordinates to CSV and JSON.\")\n\n#     # Plot C1' atoms with backbone\n#     fig = plt.figure(figsize=(10, 8))\n#     ax = fig.add_subplot(111, projection='3d')\n\n#     chain_ids = np.unique(c1_atoms.chain_id)\n#     colors = plt.cm.rainbow(np.linspace(0, 1, len(chain_ids)))\n\n#     for i, chain_id in enumerate(chain_ids):\n#         chain_mask = c1_atoms.chain_id == chain_id\n#         chain_df = df[df[\"chain_id\"] == chain_id]\n#         chain_df_sorted = chain_df.sort_values(\"res_id\")\n\n#         ax.scatter(\n#             chain_df_sorted[\"x\"],\n#             chain_df_sorted[\"y\"],\n#             chain_df_sorted[\"z\"],\n#             c=[colors[i]],\n#             label=f\"Chain {chain_id}\",\n#             alpha=0.9,\n#             s=30\n#         )\n\n#         ax.plot(\n#             chain_df_sorted[\"x\"],\n#             chain_df_sorted[\"y\"],\n#             chain_df_sorted[\"z\"],\n#             color=colors[i],\n#             alpha=0.6,\n#             linewidth=2\n#         )\n\n#     ax.set_xlabel(\"X (Å)\")\n#     ax.set_ylabel(\"Y (Å)\")\n#     ax.set_zlabel(\"Z (Å)\")\n#     ax.set_title(f\"C1' atom backbone - {cif_files[0]}\")\n#     ax.legend()\n#     plt.tight_layout()\n#     plt.show()\n\n#     # --- NEW: Plot all atoms in another image ---\n#     print(\"\\nGenerating full atom visualization...\")\n\n#     all_coords = atom_array.coord\n#     all_chain_ids = np.unique(atom_array.chain_id)\n#     colors_all = plt.cm.rainbow(np.linspace(0, 1, len(all_chain_ids)))\n\n#     fig_all = plt.figure(figsize=(10, 8))\n#     ax_all = fig_all.add_subplot(111, projection='3d')\n\n#     for i, chain_id in enumerate(all_chain_ids):\n#         chain_mask = atom_array.chain_id == chain_id\n#         ax_all.scatter(\n#             all_coords[chain_mask, 0],\n#             all_coords[chain_mask, 1],\n#             all_coords[chain_mask, 2],\n#             c=[colors_all[i]],\n#             label=f\"Chain {chain_id}\",\n#             alpha=0.6,\n#             s=5  # smaller for all atoms\n#         )\n\n#     ax_all.set_xlabel(\"X (Å)\")\n#     ax_all.set_ylabel(\"Y (Å)\")\n#     ax_all.set_zlabel(\"Z (Å)\")\n#     ax_all.set_title(f\"All atoms - {cif_files[0]}\")\n#     ax_all.legend()\n#     plt.tight_layout()\n#     plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:53:14.639027Z","iopub.execute_input":"2025-04-04T10:53:14.639415Z","iopub.status.idle":"2025-04-04T10:53:15.084099Z","shell.execute_reply.started":"2025-04-04T10:53:14.639385Z","shell.execute_reply":"2025-04-04T10:53:15.083226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install nglview","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:56:30.789313Z","iopub.execute_input":"2025-04-04T10:56:30.789587Z","iopub.status.idle":"2025-04-04T10:57:06.942843Z","shell.execute_reply.started":"2025-04-04T10:56:30.789566Z","shell.execute_reply":"2025-04-04T10:57:06.941693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n# import os\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Path to the summary confidence JSON file\n# confidence_file = \"/kaggle/working/output/rna_prediction/seed_42/predictions/rna_prediction_seed_42_summary_confidence_sample_0.json\"\n\n# # Read and display the file contents\n# with open(confidence_file, 'r') as f:\n#     confidence_data = json.load(f)\n\n# # Show the keys in the JSON file\n# print(\"Keys in the confidence file:\")\n# for key in confidence_data.keys():\n#     print(f\" - {key}\")\n\n# # Function to display and visualize specific metrics\n# def analyze_metric(data, metric_name):\n#     if metric_name in data:\n#         print(f\"\\n{metric_name} data:\")\n        \n#         # Handle different data types\n#         if isinstance(data[metric_name], (int, float)):\n#             print(f\"{metric_name}: {data[metric_name]}\")\n        \n#         elif isinstance(data[metric_name], list):\n#             print(f\"{metric_name} (first 5 values): {data[metric_name][:5]}\")\n            \n#             # Plot if it's a list of numbers\n#             if data[metric_name] and isinstance(data[metric_name][0], (int, float)):\n#                 plt.figure(figsize=(10, 6))\n#                 plt.plot(data[metric_name])\n#                 plt.title(f\"{metric_name} across residues\")\n#                 plt.xlabel(\"Residue index\")\n#                 plt.ylabel(metric_name)\n#                 plt.grid(True, alpha=0.3)\n#                 plt.savefig(f\"{metric_name}_plot.png\")\n#                 plt.close()\n#                 print(f\"Saved visualization to '{metric_name}_plot.png'\")\n        \n#         elif isinstance(data[metric_name], dict):\n#             # For dictionaries, show keys and sample values\n#             print(f\"{metric_name} contains {len(data[metric_name])} keys:\")\n#             for k in list(data[metric_name].keys())[:5]:\n#                 print(f\"  - {k}: {data[metric_name][k]}\")\n#     else:\n#         print(f\"\\n{metric_name} not found in the data\")\n\n# # Analyze common confidence metrics\n# analyze_metric(confidence_data, \"plddt\")  # Per-residue confidence scores\n# analyze_metric(confidence_data, \"ranking_score\")  # Overall ranking score of the model\n# analyze_metric(confidence_data, \"gpde\")  # Global predicted distance error\n\n# # If there's PAE (Predicted Aligned Error) matrix, visualize it\n# if \"pae\" in confidence_data and isinstance(confidence_data[\"pae\"], list):\n#     pae_data = np.array(confidence_data[\"pae\"])\n    \n#     if len(pae_data.shape) == 2:\n#         plt.figure(figsize=(10, 8))\n#         im = plt.imshow(pae_data, cmap='viridis_r')\n#         plt.colorbar(im, label=\"Predicted Aligned Error (Å)\")\n#         plt.title(\"PAE Matrix\")\n#         plt.xlabel(\"Residue\")\n#         plt.ylabel(\"Residue\")\n#         plt.savefig(\"pae_matrix.png\")\n#         plt.close()\n#         print(\"\\nSaved PAE matrix visualization to 'pae_matrix.png'\")\n\n# # Display the structure's overall quality assessment\n# print(\"\\nOverall structure quality assessment:\")\n# quality_metrics = [\"ranking_score\", \"gpde\", \"plddt_avg\", \"pae_avg\"]\n# for metric in quality_metrics:\n#     if metric in confidence_data:\n#         print(f\" - {metric}: {confidence_data[metric]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:43:03.559591Z","iopub.execute_input":"2025-04-04T10:43:03.55982Z","iopub.status.idle":"2025-04-04T10:43:03.574372Z","shell.execute_reply.started":"2025-04-04T10:43:03.5598Z","shell.execute_reply":"2025-04-04T10:43:03.573528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n# import pprint\n\n# # Path to the summary confidence JSON file\n# confidence_file = \"/kaggle/working/output/rna_prediction/seed_42/predictions/rna_prediction_seed_42_summary_confidence_sample_0.json\"\n\n# # Read and display the complete file contents\n# with open(confidence_file, 'r') as f:\n#     confidence_data = json.load(f)\n\n# # Use pretty print to display formatted JSON\n# print(\"Complete JSON content:\")\n# pp = pprint.PrettyPrinter(indent=2)\n# pp.pprint(confidence_data)\n\n# # Or alternatively, print it with json.dumps for more control over formatting\n# print(\"\\nAlternative JSON formatting:\")\n# print(json.dumps(confidence_data, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:43:03.575142Z","iopub.execute_input":"2025-04-04T10:43:03.575459Z","iopub.status.idle":"2025-04-04T10:43:03.598221Z","shell.execute_reply.started":"2025-04-04T10:43:03.575428Z","shell.execute_reply":"2025-04-04T10:43:03.597511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}